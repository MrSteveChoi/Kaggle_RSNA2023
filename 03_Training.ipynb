{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNo9e7BVPy1lcxTVpQEy5pC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xIH6dRcVlEP0"},"outputs":[],"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n","\n","datadir = '...'\n","libdir = '.'\n","outputdir = '.'\n","otherdir = '.'\n","\n","train_bs_ = 16 # train_batch_size\n","valid_bs_ = 32 # valid_batch_size\n","num_workers_ = -1"]},{"cell_type":"markdown","source":["# Import"],"metadata":{"id":"10hWHb6C6mcI"}},{"cell_type":"code","source":["# !pip install -q git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n","\n","import sys;\n","\n","package_paths = [f'{libdir}pytorch-image-models-master']\n","for pth in package_paths:\n","    sys.path.append(pth)\n","\n","import ast\n","from glob import glob\n","import cv2\n","from skimage import io\n","import os\n","from datetime import datetime\n","import time\n","import random\n","from tqdm import tqdm\n","from contextlib import contextmanager\n","import math\n","\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","from sklearn.metrics import roc_auc_score, log_loss\n","from sklearn import metrics\n","from sklearn.model_selection import GroupKFold, StratifiedKFold, KFold\n","import torch\n","import torchvision\n","from torchvision import transforms\n","from torch import nn\n","from torch.utils.data import Dataset,DataLoader\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","from torch.nn.modules.loss import _WeightedLoss\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","from torch.optim import Adam, SGD, AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n","from warmup_scheduler import GradualWarmupScheduler\n","import timm\n","import warnings\n","import joblib\n","from scipy.ndimage.interpolation import zoom\n","import nibabel as nib\n","import pydicom as dicom\n","import gc\n","\n","\n","if CFG.device == 'TPU':\n","    !pip install -q pytorch-ignite\n","    import ignite.distributed as idist\n","elif CFG.device == 'GPU':\n","    from torch.cuda.amp import autocast, GradScaler"],"metadata":{"id":"xz75giC16p9G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CFG"],"metadata":{"id":"a18-4D54lMvB"}},{"cell_type":"code","source":["class CFG:\n","    seed=42\n","    device='GPU'\n","    nprocs=1\n","    num_workers=num_workers_\n","    train_bs=train_bs_\n","    valid_bs=valid_bs_\n","    fold_num=4\n","\n","    target_cols=['liver', 'spleen', 'left kidney', 'right kidney', 'bowel'] # 0 = background, 1 = liver, 2 = spleen, 3 = left kidney, 4 = right kidney, 5 = bowel\n","    num_classes=5\n","\n","    accum_iter=1\n","    max_grad_norm=1000\n","    print_freq=100\n","    normalize_mean=[0.4824, 0.4824, 0.4824]\n","    normalize_std=[0.22, 0.22, 0.22]\n","\n","    suffix=\"109\"\n","    fold_list=[0]\n","    epochs=15\n","    model_arch=\"efficientnet-b0\"\n","    img_size=128\n","    optimizer=\"AdamW\"\n","    scheduler=\"CosineAnnealingLR\"\n","    loss_fn=\"BCEWithLogitsLoss\"\n","    scheduler_warmup= \"GradualWarmupSchedulerV3\"\n","\n","    warmup_epo=1\n","    warmup_factor = 10\n","    T_max= epochs-warmup_epo-2 if scheduler_warmup==\"GradualWarmupSchedulerV2\" else \\\n","           epochs-warmup_epo-1 if scheduler_warmup==\"GradualWarmupSchedulerV3\" else epochs-1\n","\n","    lr=5e-3\n","    min_lr=1e-6 #\n","    weight_decay=0\n","\n","    n_early_stopping=5"],"metadata":{"id":"FuLsKXiIlMN0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"itgzh7e0nRRT"}},{"cell_type":"code","source":["class TrainDataset(Dataset):\n","    def __init__(self, train_df, transform=None):\n","        self.train_df = df\n","        self.file_names = df['id'].values\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        file_name = self.file_names[idx]\n","        image = np.load(f\"{datadir}/seg_25d_image/{file_name}.npy\") # 512 * 512 * 3\n","        mask = np.load(f\"{datadir}/seg_25d_mask/{file_name}.npy\") # 512 * 512 * 3\n","\n","        # transform\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        image = image/255.0\n","\n","\n","        real_mask = np.zeros([CFG.num_classes-1, CFG.img_size, CFG.img_size])  # 4 * img_size * img_size\n","        # liver\n","        real_mask[0] = mask[:,:,1] == 1\n","        # spleen\n","        real_mask[1] = mask[:,:,1] == 2\n","        # kidney (merge left kidney and right kidney)\n","        real_mask[2] = np.logical_or(mask[:,:,1] == 3, mask[:,:,1] == 4)\n","        # bowel\n","        real_mask[3] = mask[:,:,1] == 5\n","\n","        # for idx in range(CFG.num_classes):\n","        #     mask_bool = (mask[:,:,1] == (idx+1))\n","        #     real_mask[idx] = mask_bool\n","\n","        image = np.transpose(image, (2, 0, 1)) # 3 * img_size * img_size\n","        mask = np.transpose(mask, (2, 0, 1)) # 3 * img_size * img_size\n","\n","        return torch.from_numpy(image), torch.from_numpy(real_mask), torch.from_numpy(mask)"],"metadata":{"id":"vyD3BvQ3nQQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from albumentations import (\n","    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n","    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n","    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n","    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate,\n","    CenterCrop, Resize, RandomCrop, GaussianBlur, JpegCompression, Downscale, ElasticTransform\n",")\n","import albumentations\n","\n","from albumentations.pytorch import ToTensorV2\n","\n","def get_transforms(data):\n","    if data == 'train':\n","        return Compose([\n","            Resize(CFG.img_size, CFG.img_size, interpolation=cv2.INTER_NEAREST),\n","            HorizontalFlip(p=0.5),\n","            VerticalFlip(p=0.5),\n","            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n","            OneOf([\n","                GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n","                OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n","                ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n","            ], p=0.25),\n","            CoarseDropout(max_holes=8, max_height=CFG.img_size[0]//20, max_width=CFG.img_size[1]//20,\n","                            min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n","            ], p=1.0)\n","\n","    elif data == 'light_train':\n","        return Compose([\n","            Resize(CFG.img_size, CFG.img_size, interpolation=cv2.INTER_NEAREST),\n","            HorizontalFlip(p=0.5),\n","            # VerticalFlip(p=0.5),\n","            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n","            OneOf([\n","                GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n","                # OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n","                ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n","            ], p=0.25),\n","            # CoarseDropout(max_holes=8, max_height=CFG.img_size[0]//20, max_width=CFG.img_size[1]//20,\n","            #              min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n","            ], p=1.0)\n","\n","    elif data == 'valid':\n","        return Compose([\n","            Resize(CFG.img_size, CFG.img_size, interpolation=cv2.INTER_NEAREST),\n","        ])"],"metadata":{"id":"Tn7RbP_u1Mwc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### get_transforms(\"light_train\") 확인"],"metadata":{"id":"1Pwchk9o18Zu"}},{"cell_type":"code","source":["from pylab import rcParams\n","dataset_show = TrainDataset(\n","    train_df,\n","    get_transforms(\"light_train\") # None, get_transforms(\"train\")\n","    )\n","rcParams['figure.figsize'] = 30,20\n","for i in range(2):\n","    f, axarr = plt.subplots(1,3)\n","    idx = np.random.randint(0, len(dataset_show))\n","    img, mask, raw_mask = dataset_show[idx]\n","    # axarr[p].imshow(img) # transform=None\n","    axarr[0].imshow(img[1]); plt.axis('OFF');\n","    axarr[1].imshow(raw_mask[1]/255, alpha=0.5); plt.axis('OFF');\n","    axarr[2].imshow(img[1]); axarr[2].imshow(raw_mask[1]/255,alpha=0.5); plt.axis('OFF');"],"metadata":{"id":"8JedEHSI12Qi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"aIdxhoJx2IOn"}},{"cell_type":"code","source":["import segmentation_models_pytorch as smp\n","\n","def build_model():\n","    model = smp.Unet(\n","        encoder_name=CFG.model_arch,    # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n","        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n","        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n","        classes=CFG.num_classes-1,        # model output channels (number of classes in your dataset)\n","        activation=None,\n","    )\n","    model.to(device)\n","    return model\n","\n","def load_model(path):\n","    model = build_model()\n","    model.load_state_dict(torch.load(path))\n","    model.eval()\n","    return model"],"metadata":{"id":"jhJe877r2DE6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["JaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\n","DiceLoss    = smp.losses.DiceLoss(mode='multilabel')\n","BCELoss     = smp.losses.SoftBCEWithLogitsLoss()\n","LovaszLoss  = smp.losses.LovaszLoss(mode='multilabel', per_image=False)\n","TverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)"],"metadata":{"id":"U3khSMDL2Mx-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dice_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred>thr).to(torch.float32)\n","    inter = (y_true*y_pred).sum(dim=dim)\n","    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n","    dice = ((2*inter+epsilon)/(den+epsilon)).mean(dim=(1,0))\n","    return dice"],"metadata":{"id":"igDNtZD_7rMV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_one_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","    if CFG.device == 'GPU':\n","        scaler = GradScaler()\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    model.train()\n","    start = end = time.time()\n","    for step, (images, masks, raw_mask) in enumerate(train_loader):\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","        images = images.to(device, dtype=torch.float)\n","        masks = masks.to(device, dtype=torch.float)\n","        batch_size = images.size(0)\n","\n","        if CFG.device == 'GPU':\n","            with autocast(enabled=True):\n","                y_preds = model(images)\n","                loss = criterion(y_preds, masks)\n","            # record loss\n","            losses.update(loss.item(), batch_size)\n","            if CFG.accum_iter > 1:\n","                loss = loss / CFG.accum_iter\n","            scaler.scale(loss).backward()\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","            if (step + 1) % CFG.accum_iter == 0:\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","        elif CFG.device == 'TPU':\n","            y_preds = model(images)\n","            loss = criterion(y_preds, masks)\n","            # record loss\n","            losses.update(loss.item(), batch_size)\n","            if CFG.accum_iter > 1:\n","                loss = loss / CFG.accum_iter\n","            loss.backward()\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","            if (step + 1) % CFG.accum_iter == 0:\n","                xm.optimizer_step(optimizer, barrier=True)\n","                optimizer.zero_grad()\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            cusprint('Epoch: [{0}][{1}/{2}] '\n","                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n","                'Elapsed {remain:s} '\n","                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                'Grad: {grad_norm:.4f}  '\n","                'LR: {lr:.7f}  '\n","                .format(\n","                epoch, step, len(train_loader), batch_time=batch_time,\n","                data_time=data_time, loss=losses,\n","                remain=timeSince(start, float(step+1)/len(train_loader)),\n","                grad_norm=grad_norm,\n","                lr=optimizer.param_groups[0][\"lr\"],\n","                ))\n","\n","    return losses.avg, optimizer.param_groups[0][\"lr\"]"],"metadata":{"id":"jZH-cfpd2PRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid_one_epoch(valid_loader, model, criterion, device):\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    scores = AverageMeter()\n","    # switch to evaluation mode\n","    model.eval()\n","    start = end = time.time()\n","    val_scores = []\n","    for step, (images, masks, raw_mask) in enumerate(valid_loader):\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","        images = images.to(device, dtype=torch.float)\n","        masks = masks.to(device, dtype=torch.float)\n","        batch_size = images.size(0)\n","        # compute loss\n","        with torch.no_grad():\n","            y_pred = model(images)\n","        loss = criterion(y_pred, masks)\n","        losses.update(loss.item(), batch_size)\n","\n","        # record accuracy\n","        y_pred = y_pred.sigmoid() ####\n","        # y_pred = y_pred.sigmoid().to('cpu').numpy()\n","        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n","        val_scores.append([val_dice])\n","\n","        if CFG.accum_iter > 1:\n","            loss = loss / CFG.accum_iter\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n","            cusprint('EVAL: [{0}/{1}] '\n","                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n","                'Elapsed {remain:s} '\n","                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                .format(\n","                step, len(valid_loader), batch_time=batch_time,\n","                data_time=data_time, loss=losses,\n","                remain=timeSince(start, float(step+1)/len(valid_loader)),\n","                ))\n","\n","    val_scores = np.mean(val_scores, axis=0)\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    return losses.avg, val_scores"],"metadata":{"id":"ekG36IwF2Sz3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# loss & optimizer & scheduler"],"metadata":{"id":"If4VVMvB2alx"}},{"cell_type":"code","source":["class GradualWarmupSchedulerV3(GradualWarmupScheduler):\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        super(GradualWarmupSchedulerV3, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n","    def get_lr(self):\n","        if self.last_epoch >= self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"],"metadata":{"id":"BNP3Q4UO2b_-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"ol70gMsv2kto"}},{"cell_type":"code","source":["def train_loop(df, fold):\n","    loginfo(f\"========== fold: {fold} training ==========\")\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_folds = train_df[train_df[\"fold\"] != fold].reset_index(drop=True)\n","    valid_folds = train_df[train_df[\"fold\"] == fold].reset_index(drop=True)\n","\n","    train_dataset = TrainDataset(train_folds, transform=get_transforms(data='light_train'))\n","    valid_dataset = TrainDataset(valid_folds, transform=get_transforms(data='valid'))\n","    if CFG.device == 'GPU':\n","        train_loader = DataLoader(train_dataset, batch_size=CFG.train_bs, shuffle=True, num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","        valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs, shuffle=False, num_workers=0, pin_memory=True, drop_last=False)\n","    elif CFG.device == 'TPU':\n","        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True)\n","        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.train_bs, sampler=train_sampler, drop_last=True, num_workers=CFG.num_workers)\n","        valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False)\n","        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=CFG.valid_bs, sampler=valid_sampler, drop_last=False, num_workers=CFG.num_workers)\n","\n","    # ====================================================\n","    # model & optimizer & scheduler & loss\n","    # ====================================================\n","    model = build_model()\n","\n","    # optimizer\n","    if CFG.optimizer == \"AdamW\":\n","        if CFG.scheduler_warmup in [\"GradualWarmupSchedulerV2\",\"GradualWarmupSchedulerV3\"]:\n","            optimizer = AdamW(model.parameters(), lr=CFG.lr/CFG.warmup_factor, weight_decay=CFG.weight_decay)\n","        else:\n","            optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n","    # scheduler\n","    if CFG.scheduler=='ReduceLROnPlateau':\n","        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n","    elif CFG.scheduler=='CosineAnnealingLR':\n","        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n","    elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n","\n","    if CFG.scheduler_warmup==\"GradualWarmupSchedulerV3\":\n","        scheduler_warmup = GradualWarmupSchedulerV3(optimizer, multiplier=10, total_epoch=CFG.warmup_epo, after_scheduler=scheduler)\n","\n","    # loss\n","    def criterion(y_pred, y_true):\n","        return 0.5*BCELoss(y_pred, y_true) + 0.5*DiceLoss(y_pred, y_true)\n","\n","    def criterion_edge(y_pred, y_true):\n","      return\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","\n","    valid_acc_max=0\n","    valid_acc_max_cnt=0\n","    for epoch in range(CFG.epochs):\n","        loginfo(f\"***** Epoch {epoch} *****\")\n","        if CFG.scheduler_warmup in [\"GradualWarmupSchedulerV2\",\"GradualWarmupSchedulerV3\"]:\n","            pass\n","            # loginfo(f\"schwarmup_last_epoch:{scheduler_warmup.last_epoch}, schwarmup_lr:{scheduler_warmup.get_last_lr()[0]}\")\n","        if CFG.scheduler=='CosineAnnealingLR':\n","            loginfo(f\"scheduler_last_epoch:{scheduler.last_epoch}, scheduler_lr:{scheduler.get_last_lr()[0]}\")\n","        loginfo(f\"optimizer_lr:{optimizer.param_groups[0]['lr']}\")\n","\n","        start_time = time.time() # 记录当前时间\n","\n","        # train\n","        if CFG.device == 'TPU' and CFG.nprocs == 8:\n","            para_train_loader = pl.ParallelLoader(train_loader, [device])\n","            avg_loss, cur_lr = train_one_epoch(para_train_loader.per_device_loader(device), model, criterion, optimizer, epoch, scheduler, device)\n","        else:\n","            avg_loss, cur_lr = train_one_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","\n","        # valid\n","        if CFG.device == 'TPU' and CFG.nprocs == 8:\n","            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n","            avg_val_loss, valid_scores = valid_one_epoch(para_valid_loader.per_device_loader(device), model, criterion, device)\n","            preds = idist.all_gather(torch.tensor(preds)).to('cpu').numpy()\n","            valid_labels = idist.all_gather(torch.tensor(valid_labels)).to('cpu').numpy()\n","        else:\n","            avg_val_loss, valid_scores = valid_one_epoch(valid_loader, model, criterion, device)\n","\n","        # scoring\n","        elapsed = time.time() - start_time\n","\n","        # print(\"valid_scores:\", valid_scores, type(valid_scores))\n","        valid_scores = valid_scores[0]\n","        loginfo(f'Epoch {epoch} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        loginfo(f'Epoch {epoch} - Score: {valid_scores:.4f}')\n","\n","        if CFG.scheduler_warmup in [\"GradualWarmupSchedulerV2\",\"GradualWarmupSchedulerV3\"]:\n","            scheduler_warmup.step()\n","        elif CFG.scheduler == \"ReduceLROnPlateau\":\n","            scheduler.step(avg_val_loss)\n","        elif CFG.scheduler in [\"CosineAnnealingLR\", \"CosineAnnealingWarmRestarts\"]:\n","            scheduler.step()\n","\n","        if CFG.device == 'GPU':\n","            torch.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')\n","        elif CFG.device == 'TPU':\n","            xm.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')\n","\n","        # early stopping\n","        if valid_scores > valid_acc_max:\n","            valid_acc_max = valid_scores\n","            valid_acc_max_cnt=0\n","            best_acc_epoch = epoch\n","        else:\n","            valid_acc_max_cnt+=1\n","\n","\n","        if valid_acc_max_cnt >= CFG.n_early_stopping:\n","            if CFG.device == 'GPU':\n","                torch.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')\n","            elif CFG.device == 'TPU':\n","                xm.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')\n","            print(\"early_stopping\")\n","            break\n","\n","        if CFG.device == 'GPU':\n","            torch.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')\n","        elif CFG.device == 'TPU':\n","            xm.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')"],"metadata":{"id":"0EeboB6P2kXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    for fold in range(CFG.fold_num):\n","        if fold in CFG.fold_list:\n","            train_loop(train_df, fold)"],"metadata":{"id":"OIYGfyqG2p5z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main"],"metadata":{"id":"mwsVX6p-2ryb"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    print(CFG.suffix)\n","    if CFG.device == 'TPU':\n","        def _mp_fn(rank, flags):\n","            torch.set_default_tensor_type('torch.FloatTensor')\n","            a = main()\n","        FLAGS = {}\n","        xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=CFG.nprocs, start_method='fork')\n","    elif CFG.device == 'GPU':\n","        main()"],"metadata":{"id":"RjEog2782tBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save as cpu\n","if CFG.device == 'TPU':\n","    for fold in range(CFG.fold_num):\n","        if fold in CFG.fold_list:\n","            # best score\n","            state = torch.load(outputdir+f'{CFG.model_arch}_{CFG.suffix}_fold{fold}.pth')\n","            torch.save({'model': state['model'].to('cpu').state_dict(), 'preds': state['preds'], 'cur_best_list': state['cur_best_list']},\n","                    outputdir+f'{CFG.model_arch}_{CFG.suffix}_fold{fold}.pth')"],"metadata":{"id":"O3-pDT_g2wxY"},"execution_count":null,"outputs":[]}]}